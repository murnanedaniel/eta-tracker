{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing binned pseudorapidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint as pp\n",
    "from time import time as tt\n",
    "import pickle\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import numpy.lib.recfunctions as rfn\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Limit CPU usage on Jupyter\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Pick up local packages\n",
    "sys.path.append('..')\n",
    "\n",
    "# Profiling\n",
    "# %load_ext heat\n",
    "\n",
    "# Locals\n",
    "\n",
    "from prepareTracks import *\n",
    "from nb_utils import *\n",
    "from gpu_utils import *\n",
    "from datasets import get_data_loaders\n",
    "from trainers import get_trainer\n",
    "import distributed\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_100_events/'\n",
    "output_dir = '/global/u2/d/danieltm/ExaTrkX/eta-tracker/data/numba_testing'\n",
    "n_files = 1\n",
    "n_workers = 1\n",
    "config = {'selection': {'pt_min': 0.5,\n",
    "    'phi_slope_max': 0.0006,\n",
    "    'z0_max': 150,\n",
    "    'n_phi_sections': 1,\n",
    "    'n_eta_sections': 1,\n",
    "    'eta_range': [-5, 5]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(input_dir)\n",
    "suffix = '-hits.csv'\n",
    "file_prefixes = sorted(os.path.join(input_dir, f.replace(suffix, ''))\n",
    "                       for f in all_files if f.endswith(suffix))\n",
    "file_prefixes = np.array(file_prefixes[:n_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Numpy Threading (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../prepareTracks.py:213: RuntimeWarning: invalid value encountered in arccos\n",
      "  theta = np.arccos(p_zr[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         781 function calls in 58.327 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "       35   58.206    1.663   58.206    1.663 {method 'acquire' of '_thread.lock' objects}\n",
       "        1    0.088    0.088    0.088    0.088 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
       "        3    0.019    0.006    0.019    0.006 {built-in method posix.waitpid}\n",
       "        1    0.010    0.010    0.010    0.010 {built-in method posix.fork}\n",
       "        3    0.001    0.000    0.001    0.000 {built-in method _thread.start_new_thread}\n",
       "        1    0.000    0.000   58.327   58.327 <string>:2(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.mkdir}\n",
       "        4    0.000    0.000    0.000    0.000 synchronize.py:50(__init__)\n",
       "        1    0.000    0.000   58.327   58.327 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.014    0.014 pool.py:153(__init__)\n",
       "        1    0.000    0.000    0.010    0.010 popen_fork.py:63(_launch)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:757(__init__)\n",
       "        6    0.000    0.000   58.206    9.701 threading.py:263(wait)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:151(__init__)\n",
       "        1    0.000    0.000    0.011    0.011 process.py:95(start)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
       "       10    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
       "        9    0.000    0.000    0.000    0.000 threading.py:215(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'poll' of 'select.poll' objects}\n",
       "        6    0.000    0.000   58.206    9.701 threading.py:533(wait)\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:109(remove)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method posix.pipe}\n",
       "        1    0.000    0.000    0.000    0.000 pool.py:368(_map_async)\n",
       "        4    0.000    0.000    0.000    0.000 _weakrefset.py:81(add)\n",
       "        2    0.000    0.000    0.001    0.000 iostream.py:334(flush)\n",
       "       32    0.000    0.000    0.000    0.000 random.py:223(_randbelow)\n",
       "        2    0.000    0.000    0.107    0.054 util.py:167(__call__)\n",
       "        1    0.000    0.000    0.107    0.107 pool.py:561(_terminate_pool)\n",
       "        6    0.000    0.000    0.000    0.000 threading.py:498(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 threading.py:1230(current_thread)\n",
       "        1    0.000    0.000    0.014    0.014 context.py:114(Pool)\n",
       "        3    0.000    0.000    0.001    0.000 threading.py:828(start)\n",
       "        4    0.000    0.000    0.000    0.000 iostream.py:195(schedule)\n",
       "        1    0.000    0.000    0.011    0.011 pool.py:225(_repopulate_pool)\n",
       "        1    0.000    0.000    0.000    0.000 pool.py:663(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 socket.py:337(send)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method posix.close}\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:165(__setitem__)\n",
       "        4    0.000    0.000    0.000    0.000 tempfile.py:157(__next__)\n",
       "        1    0.000    0.000    0.011    0.011 context.py:274(_Popen)\n",
       "        1    0.000    0.000    0.000    0.000 reduction.py:38(__init__)\n",
       "       11    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:334(__new__)\n",
       "        4    0.000    0.000    0.000    0.000 context.py:64(Lock)\n",
       "        1    0.000    0.000    0.088    0.088 pool.py:552(_help_stuff_finish)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "        1    0.000    0.000    0.000    0.000 os.py:195(makedirs)\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:104(split)\n",
       "        4    0.000    0.000    0.000    0.000 synchronize.py:161(__init__)\n",
       "        1    0.000    0.000    0.011    0.011 popen_fork.py:16(__init__)\n",
       "        3    0.000    0.000    0.019    0.006 popen_fork.py:24(poll)\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "       32    0.000    0.000    0.000    0.000 random.py:255(choice)\n",
       "        2    0.000    0.000    0.001    0.000 context.py:109(SimpleQueue)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:71(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:727(_newname)\n",
       "        8    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:897(wait)\n",
       "        1    0.000    0.000    0.000    0.000 queue.py:115(put)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:136(register_after_fork)\n",
       "        1    0.000    0.000    0.000    0.000 pool.py:619(__init__)\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        5    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
       "        5    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 genericpath.py:39(isdir)\n",
       "        4    0.000    0.000    0.000    0.000 connection.py:130(__del__)\n",
       "        4    0.000    0.000    0.000    0.000 __init__.py:8(_make_name)\n",
       "       16    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 tempfile.py:160(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 synchronize.py:97(__exit__)\n",
       "        2    0.000    0.000    0.001    0.000 queues.py:313(__init__)\n",
       "       11    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "        1    0.000    0.000    0.000    0.000 threading.py:334(notify)\n",
       "        1    0.000    0.000    0.000    0.000 queue.py:27(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:390(_send_bytes)\n",
       "        1    0.000    0.000    0.107    0.107 pool.py:537(terminate)\n",
       "        7    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "        4    0.000    0.000    0.000    0.000 _weakrefset.py:38(_remove)\n",
       "        4    0.000    0.000    0.000    0.000 tempfile.py:146(rng)\n",
       "        1    0.000    0.000    0.000    0.000 reduction.py:48(dumps)\n",
       "        2    0.000    0.000    0.000    0.000 connection.py:501(Pipe)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:346(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1120(daemon)\n",
       "       62    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
       "        4    0.000    0.000    0.000    0.000 connection.py:117(__init__)\n",
       "       15    0.000    0.000    0.000    0.000 util.py:48(debug)\n",
       "        1    0.000    0.000   58.205   58.205 pool.py:637(get)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:208(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 queues.py:339(put)\n",
       "        7    0.000    0.000    0.000    0.000 threading.py:242(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:181(send_bytes)\n",
       "        1    0.000    0.000    0.000    0.000 popen_fork.py:53(terminate)\n",
       "       44    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:233(register)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:350(register)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1136(daemon)\n",
       "        1    0.000    0.000    0.001    0.001 util.py:395(_flush_std_streams)\n",
       "        1    0.000    0.000    0.001    0.001 pool.py:248(_setup_queues)\n",
       "        1    0.000    0.000   58.205   58.205 pool.py:261(map)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _weakref._remove_dead_weakref}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.kill}\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:997(_handle_fromlist)\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:339(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 threading.py:239(__enter__)\n",
       "        1    0.000    0.000    0.018    0.018 process.py:118(join)\n",
       "        1    0.000    0.000    0.000    0.000 pool.py:150(Process)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.write}\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:20(_fileobj_to_fd)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:268(close)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:365(select)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:966(_stop)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1024(join)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1894(info)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:365(_send)\n",
       "        4    0.000    0.000    0.000    0.000 connection.py:360(_close)\n",
       "        1    0.000    0.000    0.000    0.000 synchronize.py:94(__enter__)\n",
       "        5    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "        6    0.000    0.000    0.000    0.000 threading.py:248(_release_save)\n",
       "       18    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:52(_cleanup)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:128(is_alive)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:181(exitcode)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:413(_poll)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'dump' of '_pickle.Pickler' objects}\n",
       "        1    0.000    0.000    0.107    0.107 pool.py:610(__exit__)\n",
       "        1    0.000    0.000   58.205   58.205 pool.py:634(wait)\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x555555848e00}\n",
       "       32    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "        1    0.000    0.000    0.000    0.000 genericpath.py:16(exists)\n",
       "        1    0.000    0.000    0.000    0.000 <string>:12(__new__)\n",
       "        7    0.000    0.000    0.000    0.000 threading.py:254(_is_owned)\n",
       "        1    0.000    0.000    0.000    0.000 context.py:232(get_context)\n",
       "        3    0.000    0.000    0.000    0.000 connection.py:134(_check_closed)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:112(terminate)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:253(poll)\n",
       "        4    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)\n",
       "        1    0.000    0.000    0.000    0.000 pool.py:627(ready)\n",
       "        1    0.000    0.000    0.018    0.018 popen_fork.py:43(wait)\n",
       "        7    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'register' of 'select.poll' objects}\n",
       "        1    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:62(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:201(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:214(_fileobj_lookup)\n",
       "        6    0.000    0.000    0.000    0.000 threading.py:251(_acquire_restore)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1298(info)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1530(getEffectiveLevel)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1544(isEnabledFor)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 queue.py:199(_init)\n",
       "        6    0.000    0.000    0.000    0.000 context.py:186(get_context)\n",
       "        4    0.000    0.000    0.000    0.000 context.py:196(get_start_method)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:138(_check_readable)\n",
       "        2    0.000    0.000    0.000    0.000 process.py:83(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:162(daemon)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:190(ident)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:44(sub_debug)\n",
       "        1    0.000    0.000    0.000    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}\n",
       "        4    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'getbuffer' of '_io.BytesIO' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.WIFSIGNALED}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.WEXITSTATUS}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'locked' of '_thread.lock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _imp.lock_held}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method select.poll}\n",
       "        1    0.000    0.000    0.000    0.000 selectors.py:198(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 queue.py:206(_put)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:142(_check_writable)\n",
       "        1    0.000    0.000    0.000    0.000 connection.py:168(fileno)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:146(name)\n",
       "        1    0.000    0.000    0.000    0.000 process.py:150(name)\n",
       "        1    0.000    0.000    0.000    0.000 pool.py:607(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISDIR}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.WIFEXITED}\n",
       "        9    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Prepare output\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logging.info('Writing outputs to ' + output_dir)\n",
    "\n",
    "# Process input files with a worker pool\n",
    "with mp.Pool(processes=n_workers) as pool:\n",
    "    process_func = partial(process_event, output_dir=output_dir,\n",
    "                           phi_range=(-np.pi, np.pi), **config['selection'])\n",
    "    pool.map(process_func, file_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\n[1] During: typing of argument at <ipython-input-37-185a1b861a1a> (11)\n\nFile \"<ipython-input-37-185a1b861a1a>\", line 11:\ndef calc_eta(r, z):\n    theta = np.arctan2(r, z)\n    ^\n\nThis error may have been caused by the following argument(s):\n- argument 0: cannot determine Numba type of <class 'pandas.core.series.Series'>\n- argument 1: cannot determine Numba type of <class 'pandas.core.series.Series'>\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/latest/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/latest/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-36-d1bada84162d>\", line 238, in process_event\n    hits_sections = split_detector_sections(hits, phi_edges, eta_edges)\n  File \"<ipython-input-36-d1bada84162d>\", line 157, in split_detector_sections\n    eta = calc_eta(phi_hits.r, phi_hits.z)\n  File \"/global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/dispatcher.py\", line 401, in _compile_for_args\n    error_rewrite(e, 'typing')\n  File \"/global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/dispatcher.py\", line 344, in error_rewrite\n    reraise(type(e), e, None)\n  File \"/global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/six.py\", line 668, in reraise\n    raise value.with_traceback(tb)\nnumba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\n[1] During: typing of argument at <ipython-input-37-185a1b861a1a> (11)\n\nFile \"<ipython-input-37-185a1b861a1a>\", line 11:\ndef calc_eta(r, z):\n    theta = np.arctan2(r, z)\n    ^\n\nThis error may have been caused by the following argument(s):\n- argument 0: cannot determine Numba type of <class 'pandas.core.series.Series'>\n- argument 1: cannot determine Numba type of <class 'pandas.core.series.Series'>\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/latest/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/latest/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\n[1] During: typing of argument at <ipython-input-37-185a1b861a1a> (11)\n\nFile \"<ipython-input-37-185a1b861a1a>\", line 11:\ndef calc_eta(r, z):\n    theta = np.arctan2(r, z)\n    ^\n\nThis error may have been caused by the following argument(s):\n- argument 0: cannot determine Numba type of <class 'pandas.core.series.Series'>\n- argument 1: cannot determine Numba type of <class 'pandas.core.series.Series'>\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/latest/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/latest/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prepare output\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logging.info('Writing outputs to ' + output_dir)\n",
    "\n",
    "# Process input files with a worker pool\n",
    "with mp.Pool(processes=n_workers) as pool:\n",
    "    process_func = partial(process_event, output_dir=output_dir,\n",
    "                           phi_range=(-np.pi, np.pi), **config['selection'])\n",
    "    pool.map(process_func, file_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_dphi(phi1, phi2):\n",
    "    \"\"\"Computes phi2-phi1 given in range [-pi,pi]\"\"\"\n",
    "    dphi = phi2 - phi1\n",
    "    dphi[dphi > np.pi] -= 2*np.pi\n",
    "    dphi[dphi < -np.pi] += 2*np.pi\n",
    "    return dphi\n",
    "\n",
    "@njit\n",
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    return -1. * np.log(np.tan(theta / 2.))\n",
    "\n",
    "def select_segments(hits1, hits2, phi_slope_max, z0_max):\n",
    "    \"\"\"\n",
    "    Construct a list of selected segments from the pairings\n",
    "    between hits1 and hits2, filtered with the specified\n",
    "    phi slope and z0 criteria.\n",
    "\n",
    "    Returns: pd DataFrame of (index_1, index_2), corresponding to the\n",
    "    DataFrame hit label-indices in hits1 and hits2, respectively.\n",
    "    \"\"\"\n",
    "    # Start with all possible pairs of hits\n",
    "    keys = ['evtid', 'r', 'phi', 'z']\n",
    "    hit_pairs = hits1[keys].reset_index().merge(\n",
    "        hits2[keys].reset_index(), on='evtid', suffixes=('_1', '_2'))\n",
    "    # Compute line through the points\n",
    "    dphi = calc_dphi(hit_pairs.phi_1, hit_pairs.phi_2)\n",
    "    dz = hit_pairs.z_2 - hit_pairs.z_1\n",
    "    dr = hit_pairs.r_2 - hit_pairs.r_1\n",
    "    phi_slope = dphi / dr\n",
    "    z0 = hit_pairs.z_1 - hit_pairs.r_1 * dz / dr\n",
    "    # Filter segments according to criteria\n",
    "    good_seg_mask = (phi_slope.abs() < phi_slope_max) & (z0.abs() < z0_max)\n",
    "    return hit_pairs[['index_1', 'index_2']][good_seg_mask]\n",
    "\n",
    "def construct_graph(hits, layer_pairs,\n",
    "                    phi_slope_max, z0_max,\n",
    "                    feature_names, feature_scale, T_feature_names, T_feature_scale, max_tracks=40):\n",
    "    \"\"\"Construct one graph (e.g. from one event)\"\"\"\n",
    "\n",
    "    # Loop over layer pairs and construct segments\n",
    "    layer_groups = hits.groupby('layer')\n",
    "    segments = []\n",
    "    for (layer1, layer2) in layer_pairs:\n",
    "        # Find and join all hit pairs\n",
    "        try:\n",
    "            hits1 = layer_groups.get_group(layer1)\n",
    "            hits2 = layer_groups.get_group(layer2)\n",
    "        # If an event has no hits on a layer, we get a KeyError.\n",
    "        # In that case we just skip to the next layer pair\n",
    "        except KeyError as e:\n",
    "            logging.info('skipping empty layer: %s' % e)\n",
    "            continue\n",
    "        # Construct the segments\n",
    "        segments.append(select_segments(hits1, hits2, phi_slope_max, z0_max))\n",
    "    # Combine segments from all layer pairs\n",
    "    segments = pd.concat(segments)\n",
    "\n",
    "    # Prepare the graph matrices\n",
    "    n_hits = hits.shape[0]\n",
    "    n_edges = segments.shape[0]\n",
    "    unique_pids = pd.unique(hits['particle_id'])\n",
    "    \n",
    "    X = (hits[feature_names].values / feature_scale).astype(np.float32)\n",
    "    Ri = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    Ro = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    y_edges = np.zeros(n_edges, dtype=np.float32)\n",
    "        \n",
    "    I = hits['hit_id']\n",
    "\n",
    "    # We have the segments' hits given by dataframe label,\n",
    "    # so we need to translate into positional indices.\n",
    "    # Use a series to map hit label-index onto positional-index.\n",
    "    hit_idx = pd.Series(np.arange(n_hits), index=hits.index)\n",
    "    seg_start = hit_idx.loc[segments.index_1].values\n",
    "    seg_end = hit_idx.loc[segments.index_2].values\n",
    "\n",
    "    # Now we can fill the association matrices.\n",
    "    # Note that Ri maps hits onto their incoming edges,\n",
    "    # which are actually segment endings.\n",
    "    Ri[seg_end, np.arange(n_edges)] = 1\n",
    "    Ro[seg_start, np.arange(n_edges)] = 1\n",
    "    # Fill the segment labels\n",
    "    pid1 = hits.particle_id.loc[segments.index_1].values\n",
    "    pid2 = hits.particle_id.loc[segments.index_2].values\n",
    "    \n",
    "    y_edges[:] = (pid1 == pid2)\n",
    "    pids = pid1*y_edges\n",
    "    \n",
    "    hits = hits.assign(**{'{}'.format(k):v for k,v in zip(T_feature_names[:5], np.zeros(len(hits)))})\n",
    "    \n",
    "    # Calculate track parameters\n",
    "    for pid in unique_pids:\n",
    "        hits.loc[hits.particle_id == pid, T_feature_names[:5]] = get_track_parameters(hits[hits.particle_id == pid]['x'].to_numpy(), hits[hits.particle_id == pid]['y'].to_numpy(), hits[hits.particle_id == pid]['z'].to_numpy())\n",
    "    \n",
    "    y_params = (hits[T_feature_names].values / T_feature_scale).astype(np.float32)\n",
    "    \n",
    "#     print(\"Finished constructing graph\")\n",
    "    # Return a tuple of the results\n",
    "    return Graph(X, Ri, Ro, y_edges, y_params, pids), I\n",
    "\n",
    "def select_hits(hits, truth, particles, pt_min=0):\n",
    "    # Barrel volume and layer ids\n",
    "    vlids = [(8,2), (8,4), (8,6), (8,8),\n",
    "             (13,2), (13,4), (13,6), (13,8),\n",
    "             (17,2), (17,4)]\n",
    "    \n",
    "    # Assign volume IDs\n",
    "    s = 0\n",
    "    layer_dic = {}\n",
    "    for v in vlids:\n",
    "        if v[0] not in layer_dic: layer_dic[v[0]] = {}\n",
    "        layer_dic[v[0]].update({v[1] : s})\n",
    "        s += 1\n",
    "    volume_mask = np.isin(hits['volume_id'], list(layer_dic.keys()))\n",
    "    hits = hits[volume_mask]\n",
    "    layer_list = [layer_dic[row['volume_id']][row['layer_id']] for row in hits]\n",
    "    hits['layer_id'] = layer_list\n",
    "    hits = hits_vlid\n",
    "    \n",
    "    # Apply momentum cut\n",
    "    particles = particles[np.sqrt(particles.px**2 + particles.py**2) > pt_min]\n",
    "    pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "    rfn.append_fields(particles, 'pt', pt, usemask=False)\n",
    "    \n",
    "    # Combine particles with truth vector\n",
    "    filler_particles = np.zeros((len(truth),), dtype=[('q', '<i4'), ('pt', '<f4')])\n",
    "    for pi in np.unique(particles['particle_id']):\n",
    "        filler_particles[truth['particle_id'] == pi] = particles[particles['particle_id'] == pi][['q', 'pt']]\n",
    "    truth = truth[filler_particles['pt'] > 0]\n",
    "    filler_particles = filler_particles[filler_particles['pt']>0]\n",
    "    truth = rfn.rec_append_fields(truth, ['q', 'pt'], [filler_particles['q'], filler_particles['pt']])\n",
    "    \n",
    "    # Select the data columns we need\n",
    "    hits = rfn.join_by('hit_id', rfn.drop_fields(hits, ['volume_id', 'module_id'], usemask=False), rfn.drop_fields(truth, ['weight'], usemask=False), usemask=False)\n",
    "    r = np.sqrt(hits['y']**2 + hits['x']**2)\n",
    "    phi = np.arctan2(hits['y'], hits['x'])\n",
    "    hits = rfn.append_fields(hits, ['r', 'phi'], [r,phi], usemask=False)\n",
    "    \n",
    "    # Remove duplicate hits\n",
    "    hits = hits.loc[\n",
    "        hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
    "    ]\n",
    "    \n",
    "    # Remove tracks with less than 3 hits\n",
    "    pid_counts = hits.groupby(['particle_id']).size().to_frame(name = 'pidcount').reset_index()\n",
    "    pid_counts = pid_counts[pid_counts.pidcount > 2]    \n",
    "    \n",
    "    hits = hits.loc[hits['particle_id'].isin(pid_counts['particle_id'])]\n",
    "    \n",
    "    #This also removes tracks with less than 3 hits, but doesn't handle duplicates\n",
    "#     hits = hits[hits['nhits'] > 2]\n",
    "#     print(\"Finished selecting hits\")\n",
    "    return hits\n",
    "\n",
    "def split_detector_sections(hits, phi_edges, eta_edges):\n",
    "    \"\"\"Split hits according to provided phi and eta boundaries.\"\"\"\n",
    "    hits_sections = []\n",
    "    # Loop over sections\n",
    "    for i in range(len(phi_edges) - 1):\n",
    "        phi_min, phi_max = phi_edges[i], phi_edges[i+1]\n",
    "        # Select hits in this phi section\n",
    "        phi_hits = hits[(hits.phi > phi_min) & (hits.phi < phi_max)]\n",
    "        # Center these hits on phi=0\n",
    "        centered_phi = phi_hits.phi - (phi_min + phi_max) / 2\n",
    "        phi_hits = phi_hits.assign(phi=centered_phi, phi_section=i)\n",
    "        for j in range(len(eta_edges) - 1):\n",
    "            eta_min, eta_max = eta_edges[j], eta_edges[j+1]\n",
    "            # Select hits in this eta section\n",
    "            eta = calc_eta(phi_hits.r, phi_hits.z)\n",
    "            sec_hits = phi_hits[(eta > eta_min) & (eta < eta_max)]\n",
    "            hits_sections.append(sec_hits.assign(eta_section=j))\n",
    "    return hits_sections\n",
    "\n",
    "@njit  \n",
    "def quadratic_formula(a, b, c):\n",
    "    if a == 0:\n",
    "        return (-c/b, )\n",
    "    x1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "    x2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "    return (x1, x2)\n",
    "\n",
    "@njit\n",
    "def calc_R(xc, yc):\n",
    "    return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "@njit\n",
    "def fnc(c):\n",
    "    Ri = calc_R(*c)\n",
    "    return Ri - Ri.mean()\n",
    "\n",
    "@vectorize\n",
    "def get_track_parameters(x, y, z):\n",
    "    # find the center of helix in x-y plane\n",
    "\n",
    "    r3 = np.sqrt(x**2 + y**2 + z**2)\n",
    "    p_zr0 = np.polyfit(r3, z, 1, full=True)\n",
    "    res0 = p_zr0[1][0]/x.shape[0]\n",
    "    p_zr = p_zr0[0]\n",
    "\n",
    "    theta = np.arccos(p_zr[0])\n",
    "    # theta = np.arccos(z[0]/r3[0])\n",
    "    eta = -np.log(np.tan(theta/2.))\n",
    "\n",
    "    center_estimate = np.mean(x), np.mean(y)\n",
    "    trans_center, ier = optimize.leastsq(fnc, center_estimate)\n",
    "    x0, y0 = trans_center\n",
    "    R = calc_R(*trans_center).mean()\n",
    "\n",
    "    # d0, z0\n",
    "    d0 = abs(np.sqrt(x0**2 + y0**2) - R)\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    p_rz = np.polyfit(r, z, 1)\n",
    "    pp_rz = np.poly1d(p_rz)\n",
    "    z0 = pp_rz(d0)\n",
    "\n",
    "    # find the closest approaching point in x-y plane\n",
    "    int_a = 1 + y0**2/x0**2\n",
    "    int_b = -2*(x0 + y0**2/x0)\n",
    "    int_c = x0**2 + y0**2 - R**2\n",
    "    int_x0, int_x1 = quadratic_formula(int_a, int_b, int_c)\n",
    "    x1 = int_x0 if abs(int_x0) < abs(int_x1) else int_x1\n",
    "    y1 = y0*x1/x0\n",
    "    phi = np.arctan2(y1, x1)\n",
    "\n",
    "    # track travels colockwise or anti-colockwise\n",
    "    # positive for colckwise\n",
    "    xs = x[0] if x[0] != 0 else 1e-1\n",
    "    ys = y[0] if y[0] != 0 else 1e-1\n",
    "    is_14 = xs > 0\n",
    "    is_above = y0 > ys/xs*x0\n",
    "    sgn = 1 if is_14^is_above else -1\n",
    "\n",
    "    # last entry is pT*(charge sign)\n",
    "    return (d0, z0, phi, eta, 0.6*sgn*R/1000)\n",
    "\n",
    "def process_event(prefix, output_dir, pt_min, n_eta_sections, n_phi_sections,\n",
    "                  eta_range, phi_range, phi_slope_max, z0_max):\n",
    "    # Load the data\n",
    "    evtid = int(prefix[-9:])\n",
    "    logging.info('Event %i, loading data' % evtid)\n",
    "#     print('Event %i, loading data' % evtid)\n",
    "    hits, particles, truth = trackml.dataset.load_event(\n",
    "        prefix, parts=['hits', 'particles', 'truth'])\n",
    "\n",
    "    # Convert Pandas to Numpy\n",
    "    hits, particles, truth = hits.to_records(index=False), particles.to_records(index=False), truth.to_records(index=False)\n",
    "    \n",
    "    # Apply hit selection\n",
    "    logging.info('Event %i, selecting hits' % evtid)\n",
    "    hits = select_hits(hits, truth, particles, pt_min=pt_min).assign(evtid=evtid)\n",
    "\n",
    "    # Divide detector into sections\n",
    "    #phi_range = (-np.pi, np.pi)\n",
    "    phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)\n",
    "    eta_edges = np.linspace(*eta_range, num=n_eta_sections+1)\n",
    "    hits_sections = split_detector_sections(hits, phi_edges, eta_edges)\n",
    "\n",
    "    # Geometric features and scale\n",
    "    feature_names = 'r phi z'.split()\n",
    "    d = dict((i,n) for n,i in enumerate(feature_names))\n",
    "    feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])\n",
    "    \n",
    "    # Truth param features and scale\n",
    "    T_feature_names = 'd0_T z0_T phi_T eta_T pt_T pt q tpx tpy tpz'.split()\n",
    "    d = dict((i,n) for n,i in enumerate(col_names))\n",
    "    T_feature_scale = np.array([100., 200., 5., 5., 20., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    # Define adjacent layers\n",
    "    n_det_layers = 10\n",
    "    l = np.arange(n_det_layers)\n",
    "    layer_pairs = np.stack([l[:-1], l[1:]], axis=1)\n",
    "\n",
    "    # Construct the graph\n",
    "    logging.info('Event %i, constructing graphs' % evtid)\n",
    "    tic = time.time()\n",
    "    graphs_all = [construct_graph(section_hits, layer_pairs=layer_pairs,\n",
    "                              phi_slope_max=phi_slope_max, z0_max=z0_max,\n",
    "                              feature_names=feature_names,\n",
    "                              feature_scale=feature_scale, \n",
    "                                T_feature_names=T_feature_names,\n",
    "                                T_feature_scale=T_feature_scale)\n",
    "              for section_hits in hits_sections]\n",
    "    graphs = [x[0] for x in graphs_all]\n",
    "    IDs    = [x[1] for x in graphs_all]\n",
    "    # pids   = [x[2] for x in graphs_all]\n",
    "    toc = time.time()\n",
    "    logging.info('Graph construction time: %f' % (toc-tic))\n",
    "    # Write these graphs to the output directory\n",
    "    try:\n",
    "        base_prefix = os.path.basename(prefix)\n",
    "        filenames = [os.path.join(output_dir, '%s_g%03i' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "        filenames_ID = [os.path.join(output_dir, '%s_g%03i_ID' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "        filenames_pid = [os.path.join(output_dir, '%s_g%03i_pid' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "    logging.info('Event %i, writing graphs', evtid)\n",
    "    save_graphs(graphs, filenames)\n",
    "    for ID, file_name in zip(IDs, filenames_ID):\n",
    "        np.savez(file_name, ID=ID)\n",
    "    # for pid, file_name in zip(pids, filenames_pid):\n",
    "    #     np.savez(file_name, pid=pid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x = np.arange(100000000).reshape(10000, 10000)\n",
    "\n",
    "def go_slow(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0.\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "#     return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "@cuda.jit # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast_gpu(a): # Function is compiled to machine code when called the first time\n",
    "    trace = float(0.)\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += float(math.tanh(float(a[i, i]))) # Numba likes NumPy functions\n",
    "#     a = a+trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "@njit # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = float(0.)\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += float(math.tanh(float(a[i, i]))) # Numba likes NumPy functions\n",
    "#     a = a+trace              # Numba likes NumPy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 ms, sys: 0 ns, total: 15.4 ms\n",
      "Wall time: 15.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "go_slow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 3 µs, total: 8 µs\n",
      "Wall time: 10.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "go_fast(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dphi(phi1, phi2):\n",
    "    \"\"\"Computes phi2-phi1 given in range [-pi,pi]\"\"\"\n",
    "    dphi = phi2 - phi1\n",
    "    dphi[dphi > np.pi] -= 2*np.pi\n",
    "    dphi[dphi < -np.pi] += 2*np.pi\n",
    "    return dphi\n",
    "\n",
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    return -1. * np.log(np.tan(theta / 2.))\n",
    "\n",
    "def select_segments(hits1, hits2, phi_slope_max, z0_max):\n",
    "    \"\"\"\n",
    "    Construct a list of selected segments from the pairings\n",
    "    between hits1 and hits2, filtered with the specified\n",
    "    phi slope and z0 criteria.\n",
    "\n",
    "    Returns: pd DataFrame of (index_1, index_2), corresponding to the\n",
    "    DataFrame hit label-indices in hits1 and hits2, respectively.\n",
    "    \"\"\"\n",
    "    # Start with all possible pairs of hits\n",
    "    keys = ['evtid', 'r', 'phi', 'z']\n",
    "    hit_pairs = hits1[keys].reset_index().merge(\n",
    "        hits2[keys].reset_index(), on='evtid', suffixes=('_1', '_2'))\n",
    "    # Compute line through the points\n",
    "    dphi = calc_dphi(hit_pairs.phi_1, hit_pairs.phi_2)\n",
    "    dz = hit_pairs.z_2 - hit_pairs.z_1\n",
    "    dr = hit_pairs.r_2 - hit_pairs.r_1\n",
    "    phi_slope = dphi / dr\n",
    "    z0 = hit_pairs.z_1 - hit_pairs.r_1 * dz / dr\n",
    "    # Filter segments according to criteria\n",
    "    good_seg_mask = (phi_slope.abs() < phi_slope_max) & (z0.abs() < z0_max)\n",
    "    return hit_pairs[['index_1', 'index_2']][good_seg_mask]\n",
    "\n",
    "def construct_graph(hits, layer_pairs,\n",
    "                    phi_slope_max, z0_max,\n",
    "                    feature_names, feature_scale, T_feature_names, T_feature_scale, max_tracks=40):\n",
    "    \"\"\"Construct one graph (e.g. from one event)\"\"\"\n",
    "\n",
    "    # Loop over layer pairs and construct segments\n",
    "    layer_groups = hits.groupby('layer')\n",
    "    segments = []\n",
    "    for (layer1, layer2) in layer_pairs:\n",
    "        # Find and join all hit pairs\n",
    "        try:\n",
    "            hits1 = layer_groups.get_group(layer1)\n",
    "            hits2 = layer_groups.get_group(layer2)\n",
    "        # If an event has no hits on a layer, we get a KeyError.\n",
    "        # In that case we just skip to the next layer pair\n",
    "        except KeyError as e:\n",
    "            logging.info('skipping empty layer: %s' % e)\n",
    "            continue\n",
    "        # Construct the segments\n",
    "        segments.append(select_segments(hits1, hits2, phi_slope_max, z0_max))\n",
    "    # Combine segments from all layer pairs\n",
    "    segments = pd.concat(segments)\n",
    "\n",
    "    # Prepare the graph matrices\n",
    "    n_hits = hits.shape[0]\n",
    "    n_edges = segments.shape[0]\n",
    "    unique_pids = pd.unique(hits['particle_id'])\n",
    "    \n",
    "    X = (hits[feature_names].values / feature_scale).astype(np.float32)\n",
    "    Ri = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    Ro = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    y_edges = np.zeros(n_edges, dtype=np.float32)\n",
    "        \n",
    "    I = hits['hit_id']\n",
    "\n",
    "    # We have the segments' hits given by dataframe label,\n",
    "    # so we need to translate into positional indices.\n",
    "    # Use a series to map hit label-index onto positional-index.\n",
    "    hit_idx = pd.Series(np.arange(n_hits), index=hits.index)\n",
    "    seg_start = hit_idx.loc[segments.index_1].values\n",
    "    seg_end = hit_idx.loc[segments.index_2].values\n",
    "\n",
    "    # Now we can fill the association matrices.\n",
    "    # Note that Ri maps hits onto their incoming edges,\n",
    "    # which are actually segment endings.\n",
    "    Ri[seg_end, np.arange(n_edges)] = 1\n",
    "    Ro[seg_start, np.arange(n_edges)] = 1\n",
    "    # Fill the segment labels\n",
    "    pid1 = hits.particle_id.loc[segments.index_1].values\n",
    "    pid2 = hits.particle_id.loc[segments.index_2].values\n",
    "    \n",
    "    y_edges[:] = (pid1 == pid2)\n",
    "    pids = pid1*y_edges\n",
    "\n",
    "    y_params = (hits[T_feature_names].values / T_feature_scale).astype(np.float32)\n",
    "    \n",
    "#     print(\"Finished constructing graph\")\n",
    "    # Return a tuple of the results\n",
    "    return Graph(X, Ri, Ro, y_edges, y_params, pids), I\n",
    "\n",
    "def select_hits(hits, truth, particles, pt_min=0):\n",
    "    # Barrel volume and layer ids\n",
    "    vlids = [(8,2), (8,4), (8,6), (8,8),\n",
    "             (13,2), (13,4), (13,6), (13,8),\n",
    "             (17,2), (17,4)]\n",
    "    n_det_layers = len(vlids)\n",
    "    # Select barrel layers and assign convenient layer number [0-9]\n",
    "    vlid_groups = hits.groupby(['volume_id', 'layer_id'])\n",
    "    hits = pd.concat([vlid_groups.get_group(vlids[i]).assign(layer=i)\n",
    "                      for i in range(n_det_layers)])\n",
    "    # Calculate particle transverse momentum\n",
    "    pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "    # True particle selection.\n",
    "    # Applies pt cut, removes all noise hits.\n",
    "    particles = particles[pt > pt_min]\n",
    "    particles = particles.join(pd.DataFrame(pt, columns=[\"pt\"]))\n",
    "    \n",
    "    truth = (truth[['hit_id', 'particle_id', 'tpx', 'tpy', 'tpz']]\n",
    "             .merge(particles[['particle_id', 'q', 'pt']], on='particle_id'))\n",
    "    # Calculate derived hits variables\n",
    "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    phi = np.arctan2(hits.y, hits.x)\n",
    "    # Select the data columns we need\n",
    "    hits = (hits[['hit_id', 'x', 'y', 'z', 'layer']]\n",
    "            .assign(r=r, phi=phi)\n",
    "            .merge(truth[['hit_id', 'particle_id', 'pt', 'q', 'tpx', 'tpy', 'tpz']], on='hit_id'))\n",
    "    # Remove duplicate hits\n",
    "    hits = hits.loc[\n",
    "        hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
    "    ]\n",
    "    \n",
    "    # Remove tracks with less than 3 hits\n",
    "    pid_counts = hits.groupby(['particle_id']).size().to_frame(name = 'pidcount').reset_index()\n",
    "    pid_counts = pid_counts[pid_counts.pidcount > 2]    \n",
    "    \n",
    "    hits = hits.loc[hits['particle_id'].isin(pid_counts['particle_id'])]\n",
    "    \n",
    "    #This also removes tracks with less than 3 hits, but doesn't handle duplicates\n",
    "#     hits = hits[hits['nhits'] > 2]\n",
    "#     print(\"Finished selecting hits\")\n",
    "    return hits\n",
    "\n",
    "def split_detector_sections(hits, phi_edges, eta_edges):\n",
    "    \"\"\"Split hits according to provided phi and eta boundaries.\"\"\"\n",
    "    hits_sections = []\n",
    "    # Loop over sections\n",
    "    for i in range(len(phi_edges) - 1):\n",
    "        phi_min, phi_max = phi_edges[i], phi_edges[i+1]\n",
    "        # Select hits in this phi section\n",
    "        phi_hits = hits[(hits.phi > phi_min) & (hits.phi < phi_max)]\n",
    "        # Center these hits on phi=0\n",
    "        centered_phi = phi_hits.phi - (phi_min + phi_max) / 2\n",
    "        phi_hits = phi_hits.assign(phi=centered_phi, phi_section=i)\n",
    "        for j in range(len(eta_edges) - 1):\n",
    "            eta_min, eta_max = eta_edges[j], eta_edges[j+1]\n",
    "            # Select hits in this eta section\n",
    "            eta = calc_eta(phi_hits.r, phi_hits.z)\n",
    "            sec_hits = phi_hits[(eta > eta_min) & (eta < eta_max)]\n",
    "            hits_sections.append(sec_hits.assign(eta_section=j))\n",
    "    return hits_sections\n",
    "\n",
    "def get_track_parameters(x, y, z):\n",
    "    # find the center of helix in x-y plane\n",
    "    def calc_R(xc, yc):\n",
    "        return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "    def fnc(c):\n",
    "        Ri = calc_R(*c)\n",
    "        return Ri - Ri.mean()\n",
    "\n",
    "    r3 = np.sqrt(x**2 + y**2 + z**2)\n",
    "    p_zr0 = np.polyfit(r3, z, 1, full=True)\n",
    "    res0 = p_zr0[1][0]/x.shape[0]\n",
    "    p_zr = p_zr0[0]\n",
    "\n",
    "    theta = np.arccos(p_zr[0])\n",
    "    # theta = np.arccos(z[0]/r3[0])\n",
    "    eta = -np.log(np.tan(theta/2.))\n",
    "\n",
    "    center_estimate = np.mean(x), np.mean(y)\n",
    "    trans_center, ier = optimize.leastsq(fnc, center_estimate)\n",
    "    x0, y0 = trans_center\n",
    "    R = calc_R(*trans_center).mean()\n",
    "\n",
    "    # d0, z0\n",
    "    d0 = abs(np.sqrt(x0**2 + y0**2) - R)\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    p_rz = np.polyfit(r, z, 1)\n",
    "    pp_rz = np.poly1d(p_rz)\n",
    "    z0 = pp_rz(d0)\n",
    "\n",
    "\n",
    "    def quadratic_formula(a, b, c):\n",
    "        if a == 0:\n",
    "            return (-c/b, )\n",
    "        x1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "        x2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "        return (x1, x2)\n",
    "\n",
    "    # find the closest approaching point in x-y plane\n",
    "    int_a = 1 + y0**2/x0**2\n",
    "    int_b = -2*(x0 + y0**2/x0)\n",
    "    int_c = x0**2 + y0**2 - R**2\n",
    "    int_x0, int_x1 = quadratic_formula(int_a, int_b, int_c)\n",
    "    x1 = int_x0 if abs(int_x0) < abs(int_x1) else int_x1\n",
    "    y1 = y0*x1/x0\n",
    "    phi = np.arctan2(y1, x1)\n",
    "\n",
    "    # track travels colockwise or anti-colockwise\n",
    "    # positive for colckwise\n",
    "    xs = x[0] if x[0] != 0 else 1e-1\n",
    "    ys = y[0] if y[0] != 0 else 1e-1\n",
    "    is_14 = xs > 0\n",
    "    is_above = y0 > ys/xs*x0\n",
    "    sgn = 1 if is_14^is_above else -1\n",
    "\n",
    "    # last entry is pT*(charge sign)\n",
    "    return (d0, z0, phi, eta, 0.6*sgn*R/1000)\n",
    "\n",
    "def process_event(prefix, output_dir, pt_min, n_eta_sections, n_phi_sections,\n",
    "                  eta_range, phi_range, phi_slope_max, z0_max):\n",
    "    # Load the data\n",
    "    evtid = int(prefix[-9:])\n",
    "    logging.info('Event %i, loading data' % evtid)\n",
    "#     print('Event %i, loading data' % evtid)\n",
    "    hits, particles, truth = trackml.dataset.load_event(\n",
    "        prefix, parts=['hits', 'particles', 'truth'])\n",
    "\n",
    "    # Apply hit selection\n",
    "    logging.info('Event %i, selecting hits' % evtid)\n",
    "    hits = select_hits(hits, truth, particles, pt_min=pt_min).assign(evtid=evtid)\n",
    "\n",
    "    # Geometric features and scale\n",
    "    feature_names = ['r', 'phi', 'z']\n",
    "    feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])\n",
    "    \n",
    "    # Track param features and scale\n",
    "    T_feature_names = ['d0_T', 'z0_T', 'phi_T', 'eta_T', 'pt_T', 'pt', 'q', 'tpx', 'tpy', 'tpz']\n",
    "    T_feature_scale = np.array([100., 200., 5., 5., 20., 1., 1., 1., 1., 1.])\n",
    "\n",
    "    # Set up columns for new hit parameters\n",
    "    hits = hits.assign(**{'{}'.format(k):v for k,v in zip(T_feature_names[:5], np.zeros(len(hits)))})\n",
    "    \n",
    "    unique_pids = pd.unique(hits['particle_id'])\n",
    "    # Calculate track parameters\n",
    "    for pid in unique_pids:\n",
    "        hits.loc[hits.particle_id == pid, T_feature_names[:5]] = get_track_parameters(hits[hits.particle_id == pid]['x'].to_numpy(), hits[hits.particle_id == pid]['y'].to_numpy(), hits[hits.particle_id == pid]['z'].to_numpy())\n",
    "    \n",
    "    \n",
    "    # Divide detector into sections\n",
    "    #phi_range = (-np.pi, np.pi)\n",
    "    phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)\n",
    "    eta_edges = np.linspace(*eta_range, num=n_eta_sections+1)\n",
    "    hits_sections = split_detector_sections(hits, phi_edges, eta_edges)\n",
    "\n",
    "    \n",
    "    # Define adjacent layers\n",
    "    n_det_layers = 10\n",
    "    l = np.arange(n_det_layers)\n",
    "    layer_pairs = np.stack([l[:-1], l[1:]], axis=1)\n",
    "\n",
    "    # Construct the graph\n",
    "    logging.info('Event %i, constructing graphs' % evtid)\n",
    "    tic = time.time()\n",
    "    graphs_all = [construct_graph(section_hits, layer_pairs=layer_pairs,\n",
    "                              phi_slope_max=phi_slope_max, z0_max=z0_max,\n",
    "                              feature_names=feature_names,\n",
    "                              feature_scale=feature_scale, \n",
    "                                T_feature_names=T_feature_names,\n",
    "                                T_feature_scale=T_feature_scale)\n",
    "              for section_hits in hits_sections]\n",
    "    graphs = [x[0] for x in graphs_all]\n",
    "    IDs    = [x[1] for x in graphs_all]\n",
    "    # pids   = [x[2] for x in graphs_all]\n",
    "    toc = time.time()\n",
    "    logging.info('Graph construction time: %f' % (toc-tic))\n",
    "    # Write these graphs to the output directory\n",
    "    try:\n",
    "        base_prefix = os.path.basename(prefix)\n",
    "        filenames = [os.path.join(output_dir, '%s_g%03i' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "        filenames_ID = [os.path.join(output_dir, '%s_g%03i_ID' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "        filenames_pid = [os.path.join(output_dir, '%s_g%03i_pid' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "    logging.info('Event %i, writing graphs', evtid)\n",
    "    save_graphs(graphs, filenames)\n",
    "    for ID, file_name in zip(IDs, filenames_ID):\n",
    "        np.savez(file_name, ID=ID)\n",
    "    # for pid, file_name in zip(pids, filenames_pid):\n",
    "    #     np.savez(file_name, pid=pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_min, n_phi_sections, phi_slope_max, z0_max = 1, 1, 0.001, 150\n",
    "event_prefix = 'event000001000'\n",
    "hits, particles, truth = load_event(os.path.join('/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_100_events/', event_prefix), \n",
    "                                    parts=['hits', 'particles', 'truth'])\n",
    "evtid = int(event_prefix[-9:])\n",
    "hits = hits.assign(evtid=evtid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the inital constraints and choose an event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_np = rfn.append_fields(hits_vlid, 'layer_num', layer_list, usemask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hit_id', '<i4'),\n",
       " ('particle_id', '<i8'),\n",
       " ('tx', '<f4'),\n",
       " ('ty', '<f4'),\n",
       " ('tz', '<f4'),\n",
       " ('tpx', '<f4'),\n",
       " ('tpy', '<f4'),\n",
       " ('tpz', '<f4'),\n",
       " ('weight', '<f4'),\n",
       " ('q', '<i4'),\n",
       " ('pt', '<f4')]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.dtype.descr + [('q', '<i4'), ('pt', '<f4')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select end-cap volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is ONLY the end-cap\n",
    "# vlids = [(9,2), (9,4), (9,6), (9,8), (9,10), (9,12), (9,14)]\n",
    "# These are the other front detectors\n",
    "#                     (14,2), (14,4), (14,6), (14,8), (14,10), (14,12),\n",
    "#                     (18,2), (18,4), (18,6), (18,8), (18,10), (18,12)]\n",
    "# These are the barrel volumes\n",
    "vlids = [(8,2), (8,4), (8,6), (8,8),\n",
    "             (13,2), (13,4), (13,6), (13,8),\n",
    "             (17,2), (17,4)]\n",
    "# eta_region = [0,0.2]\n",
    "\n",
    "# n_det_layers = len(vlids)\n",
    "# vlid_groups = hits.groupby(['volume_id', 'layer_id'])\n",
    "# hits = pd.concat([vlid_groups.get_group(vlids[i]).assign(layer=i)\n",
    "#                       for i in range(n_det_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts to structured array\n",
    "hits, particles, truth = hits.to_records(index=False), particles.to_records(index=False), truth.to_records(index=False)\n",
    "s = 0\n",
    "layer_dic = {}\n",
    "\n",
    "# Makes unique layer labels\n",
    "for v in vlids:\n",
    "    if v[0] not in layer_dic: layer_dic[v[0]] = {}\n",
    "    layer_dic[v[0]].update({v[1] : s})\n",
    "    s += 1\n",
    "volume_mask = np.isin(hits['volume_id'], list(layer_dic.keys()))\n",
    "hits = hits[volume_mask]\n",
    "layer_list = [layer_dic[row['volume_id']][row['layer_id']] for row in hits]\n",
    "hits['layer_id'] = layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([( 16874,   -32.5544,  -3.64871, -469.865,  8, 0,    1, 1000),\n",
       "           ( 16875,   -33.1537,  -1.93474, -423.517,  8, 0,    1, 1000),\n",
       "           ( 16876,   -26.3624, -18.4237 , -461.375,  8, 0,    2, 1000),\n",
       "           ...,\n",
       "           (118006, -1021.59  ,  53.764  , 1035.4  , 17, 9, 3191, 1000),\n",
       "           (118007, -1022.28  ,  15.3443 ,  981.4  , 17, 9, 3192, 1000),\n",
       "           (118008, -1022.57  ,  13.6296 , 1057.   , 17, 9, 3192, 1000)],\n",
       "          dtype=[('hit_id', '<i4'), ('x', '<f4'), ('y', '<f4'), ('z', '<f4'), ('volume_id', '<i4'), ('layer_id', '<i4'), ('module_id', '<i4'), ('evtid', '<i8')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = particles[np.sqrt(particles.px**2 + particles.py**2) > pt_min]\n",
    "pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "particles = rfn.append_fields(particles, 'pt', pt, usemask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_particles = np.zeros((len(truth),), dtype=[('q', '<i4'), ('pt', '<f4')])\n",
    "for pi in np.unique(particles['particle_id']):\n",
    "    filler_particles[truth['particle_id'] == pi] = particles[particles['particle_id'] == pi][['q', 'pt']]\n",
    "truth = truth[filler_particles['pt'] > 0]\n",
    "filler_particles = filler_particles[filler_particles['pt']>0]\n",
    "truth = rfn.rec_append_fields(truth, ['q', 'pt'], [filler_particles['q'], filler_particles['pt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hits = np.arccos(hits.z / (hits.x**2 + hits.y**2 + hits.z**2)**(0.5))\n",
    "eta_hits = -np.log(np.tan(theta_hits/2))\n",
    "phi_hits = np.arctan2(hits.y, hits.x)\n",
    "r_hits = np.sqrt(hits.x**2 + hits.y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1383,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([( 16874,   -32.5544,  -3.64871, -469.865,  8, 0,    1, 1000),\n",
       "           ( 16875,   -33.1537,  -1.93474, -423.517,  8, 0,    1, 1000),\n",
       "           ( 16876,   -26.3624, -18.4237 , -461.375,  8, 0,    2, 1000),\n",
       "           ...,\n",
       "           (118006, -1021.59  ,  53.764  , 1035.4  , 17, 9, 3191, 1000),\n",
       "           (118007, -1022.28  ,  15.3443 ,  981.4  , 17, 9, 3192, 1000),\n",
       "           (118008, -1022.57  ,  13.6296 , 1057.   , 17, 9, 3192, 1000)],\n",
       "          dtype=[('hit_id', '<i4'), ('x', '<f4'), ('y', '<f4'), ('z', '<f4'), ('volume_id', '<i4'), ('layer_id', '<i4'), ('module_id', '<i4'), ('evtid', '<i8')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def mask_test(hits):\n",
    "    hits[hits['layer_id']==0]['particle_id'] = 7\n",
    "    return hits[hits['layer_id']==0]['particle_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nInvalid use of Function(<built-in function setitem>) with argument(s) of type(s): (unaligned array(Record(hit_id[type=int32;offset=0],x[type=float32;offset=4],y[type=float32;offset=8],z[type=float32;offset=12],layer_id[type=int32;offset=16],evtid[type=int64;offset=20],particle_id[type=int64;offset=28],tx[type=float32;offset=36],ty[type=float32;offset=40],tz[type=float32;offset=44],tpx[type=float32;offset=48],tpy[type=float32;offset=52],tpz[type=float32;offset=56],q[type=int32;offset=60],pt[type=float32;offset=64];68;False), 1d, C), Literal[str](particle_id), Literal[int](7))\n * parameterized\nIn definition 0:\n    All templates rejected with literals.\nIn definition 1:\n    All templates rejected without literals.\nIn definition 2:\n    All templates rejected with literals.\nIn definition 3:\n    All templates rejected without literals.\nIn definition 4:\n    All templates rejected with literals.\nIn definition 5:\n    All templates rejected without literals.\nIn definition 6:\n    All templates rejected with literals.\nIn definition 7:\n    All templates rejected without literals.\nIn definition 8:\n    TypeError: unsupported array index type Literal[str](particle_id) in [Literal[str](particle_id)]\n    raised from /global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/typing/arraydecl.py:71\nIn definition 9:\n    TypeError: unsupported array index type unicode_type in [unicode_type]\n    raised from /global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/typing/arraydecl.py:71\nThis error is usually caused by passing an argument of a type that is unsupported by the named function.\n[1] During: typing of staticsetitem at <ipython-input-59-1c4bb9253081> (3)\n\nFile \"<ipython-input-59-1c4bb9253081>\", line 3:\ndef mask_test(hits):\n    hits[hits['layer_id']==0]['particle_id'] = 7\n    ^\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/latest/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/latest/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0merror_rewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'typing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# Something unsupported is present in the user code, add help info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/dispatcher.py\u001b[0m in \u001b[0;36merror_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                 \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nInvalid use of Function(<built-in function setitem>) with argument(s) of type(s): (unaligned array(Record(hit_id[type=int32;offset=0],x[type=float32;offset=4],y[type=float32;offset=8],z[type=float32;offset=12],layer_id[type=int32;offset=16],evtid[type=int64;offset=20],particle_id[type=int64;offset=28],tx[type=float32;offset=36],ty[type=float32;offset=40],tz[type=float32;offset=44],tpx[type=float32;offset=48],tpy[type=float32;offset=52],tpz[type=float32;offset=56],q[type=int32;offset=60],pt[type=float32;offset=64];68;False), 1d, C), Literal[str](particle_id), Literal[int](7))\n * parameterized\nIn definition 0:\n    All templates rejected with literals.\nIn definition 1:\n    All templates rejected without literals.\nIn definition 2:\n    All templates rejected with literals.\nIn definition 3:\n    All templates rejected without literals.\nIn definition 4:\n    All templates rejected with literals.\nIn definition 5:\n    All templates rejected without literals.\nIn definition 6:\n    All templates rejected with literals.\nIn definition 7:\n    All templates rejected without literals.\nIn definition 8:\n    TypeError: unsupported array index type Literal[str](particle_id) in [Literal[str](particle_id)]\n    raised from /global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/typing/arraydecl.py:71\nIn definition 9:\n    TypeError: unsupported array index type unicode_type in [unicode_type]\n    raised from /global/homes/d/danieltm/.local/cori/pytorchv1.2.0-gpu/lib/python3.6/site-packages/numba/typing/arraydecl.py:71\nThis error is usually caused by passing an argument of a type that is unsupported by the named function.\n[1] During: typing of staticsetitem at <ipython-input-59-1c4bb9253081> (3)\n\nFile \"<ipython-input-59-1c4bb9253081>\", line 3:\ndef mask_test(hits):\n    hits[hits['layer_id']==0]['particle_id'] = 7\n    ^\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/latest/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/latest/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mask_test(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = rfn.join_by('hit_id', rfn.drop_fields(hits, ['volume_id', 'module_id'], usemask=False), rfn.drop_fields(truth, ['weight'], usemask=False), usemask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.sqrt(hits['y']**2 + hits['x']**2)\n",
    "phi = np.arctan2(hits['y'], hits['x'])\n",
    "hits = rfn.append_fields(hits, ['r', 'phi'], [r,phi], usemask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = np.dtype([('dup', int64)])\n",
    "@njit\n",
    "def rem_dup(hits):\n",
    "    dup_hits = np.zeros((len(hits),), dtype=boolean)\n",
    "    # Drop duplicates\n",
    "    layers = np.unique(hits['layer_id'])\n",
    "#     print(layers)\n",
    "    for li in prange(len(layers)):\n",
    "        matching = np.unique(hits[hits['layer_id'] == layers[li]]['particle_id'])\n",
    "        for pi in prange(len(matching)):\n",
    "            unique_hit = np.where((hits['particle_id']==matching[pi]) & (hits['layer_id']==layers[li]) & (np.min(hits[(hits['particle_id']==matching[pi]) & (hits['layer_id']==layers[li])]['r']) == hits['r']))\n",
    "            dup_hits[unique_hit] = True \n",
    "#         pass\n",
    "    return dup_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.37 s, sys: 1.25 ms, total: 2.37 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dup_hits = np.zeros((len(hits),), dtype=[('dup', 'bool')])\n",
    "# Drop duplicates\n",
    "for li in np.unique(hits['layer_id']):\n",
    "    for pi in np.unique(hits[hits['layer_id'] == li]['particle_id']):\n",
    "        dup_hits[(hits['particle_id']==pi) & (hits['layer_id']==li) & (np.min(hits[(hits['particle_id']==pi) & (hits['layer_id']==li)]['r']) == hits['r'])] = True\n",
    "new_hits = hits[dup_hits['dup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 434 ms, sys: 0 ns, total: 434 ms\n",
      "Wall time: 432 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_hits = rem_dup(hits)\n",
    "# rem_dup.parallel_diagnostics(level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6437"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(new_hits['dup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.077705, 31.579145], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0)]['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0)]['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(False,), ( True,)], dtype=[('dup', '?')])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0) & (np.min(hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0)]['r']) == hits['r'])] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0)]['r']) == hits[(hits['particle_id']==517919111108362240) & (hits['layer_id']==0)]['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create big hits + truth D.F.\n",
    "hits = (hits[['hit_id', 'x', 'y', 'z', 'layer', 'evtid']]\n",
    "            .assign(r=r_hits, phi=phi_hits, eta=eta_hits)\n",
    "            .merge(truth[['hit_id', 'particle_id', 'nhits', 'pt']], on='hit_id'))\n",
    "# Remove duplicated\n",
    "hits = hits.loc[\n",
    "        hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
    "    ]\n",
    "hits = hits[hits['nhits'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = hits.assign(**{'{}'.format(k):v for k,v in zip([\"C\", \"D\",\"E\",\"F\",\"G\"], np.zeros(len(hits)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in pd.unique(hits['particle_id']):\n",
    "    hits.loc[hits.particle_id == pid, [\"C\", \"D\", \"E\", \"F\", \"G\"]] = get_track_parameters(hits[hits.particle_id == pid]['x'].to_numpy(), hits[hits.particle_id == pid]['y'].to_numpy(), hits[hits.particle_id == pid]['z'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select certain eta region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_region = [3.2,4]\n",
    "hits = hits[(hits.eta > eta_region[0]) & (hits.eta < eta_region[1])]\n",
    "# hits = hits.sort_values(by='eta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organise into adjacent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.arange(n_det_layers)\n",
    "layer_pairs = np.stack([l[:-1], l[1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['r', 'phi', 'z']\n",
    "feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, IDs = construct_graph(hits, layer_pairs=layer_pairs,\n",
    "                              phi_slope_max=phi_slope_max, z0_max=z0_max,\n",
    "                              feature_names=feature_names,\n",
    "                              feature_scale=feature_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634  hits and  524  edges\n"
     ]
    }
   ],
   "source": [
    "Ri_rows, Ri_cols = graph.Ri.nonzero()\n",
    "Ro_rows, Ro_cols = graph.Ro.nonzero()\n",
    "n_edges = Ri_cols.shape[0]\n",
    "edge_index = np.zeros((2, n_edges), dtype=int)\n",
    "edge_index[0, Ro_cols] = Ro_rows\n",
    "edge_index[1, Ri_cols] = Ri_rows\n",
    "print(len(graph.X), \" hits and \", edge_index.shape[1], \" edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, edges, labels = graph.X, graph.y, edge_index, graph.y\n",
    "X = X*feature_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_min, n_phi_sections, phi_slope_max, z0_max = 1, 1, 0.001, 150\n",
    "event_prefix = 'event000001000'\n",
    "hits, particles, truth = load_event(os.path.join('/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_100_events/', event_prefix), \n",
    "                                    parts=['hits', 'particles', 'truth'])\n",
    "evtid = int(event_prefix[-9:])\n",
    "hits = hits.assign(evtid=evtid)\n",
    "\n",
    "# This is ONLY the end-cap\n",
    "# vlids = [(9,2), (9,4), (9,6), (9,8), (9,10), (9,12), (9,14)]\n",
    "# These are the other front detectors\n",
    "#                     (14,2), (14,4), (14,6), (14,8), (14,10), (14,12),\n",
    "#                     (18,2), (18,4), (18,6), (18,8), (18,10), (18,12)]\n",
    "# These are the barrel volumes\n",
    "vlids = [(8,2), (8,4), (8,6), (8,8),\n",
    "             (13,2), (13,4), (13,6), (13,8),\n",
    "             (17,2), (17,4)]\n",
    "# eta_region = [0,0.2]\n",
    "\n",
    "n_det_layers = len(vlids)\n",
    "vlid_groups = hits.groupby(['volume_id', 'layer_id'])\n",
    "hits = pd.concat([vlid_groups.get_group(vlids[i]).assign(layer=i)\n",
    "                      for i in range(n_det_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "particles = particles[pt > pt_min]\n",
    "particles = particles.join(pd.DataFrame(pt, columns=[\"pt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = (truth[['hit_id', 'particle_id', 'tpx', 'tpy', 'tpz']]\n",
    "             .merge(particles[['particle_id', 'q', 'pt']], on='particle_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hits = np.arccos(hits.z / (hits.x**2 + hits.y**2 + hits.z**2)**(0.5))\n",
    "eta_hits = -np.log(np.tan(theta_hits/2))\n",
    "phi_hits = np.arctan2(hits.y, hits.x)\n",
    "r_hits = np.sqrt(hits.x**2 + hits.y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create big hits + truth D.F.\n",
    "hits = (hits[['hit_id', 'x', 'y', 'z', 'layer', 'evtid']]\n",
    "        .assign(r=r_hits, phi=phi_hits, eta=eta_hits)\n",
    "        .merge(truth[['hit_id', 'particle_id', 'pt', 'q', 'tpx', 'tpy', 'tpz']], on='hit_id'))\n",
    "# Remove duplicated\n",
    "hits = hits.loc[\n",
    "        hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
    "    ]\n",
    "\n",
    "# Remove tracks with less than 3 hits\n",
    "pid_counts = hits.groupby(['particle_id']).size().to_frame(name = 'pidcount').reset_index()\n",
    "pid_counts = pid_counts[pid_counts.pidcount > 2]    \n",
    "\n",
    "hits = hits.loc[hits['particle_id'].isin(pid_counts['particle_id'])]\n",
    "# hits = hits[hits['nhits'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track param features and scale\n",
    "T_feature_names = ['d0_T', 'z0_T', 'phi_T', 'eta_T', 'pt_T', 'pt', 'q', 'tpx', 'tpy', 'tpz']\n",
    "T_feature_scale = np.array([100., 200., 5., 5., 20., 1., 1., 1., 1., 1.])\n",
    "\n",
    "# Set up columns for new hit parameters\n",
    "hits = hits.assign(**{'{}'.format(k):v for k,v in zip(T_feature_names[:5], np.zeros(len(hits)))})\n",
    "unique_pids = pd.unique(hits['particle_id'])\n",
    "# Calculate track parameters\n",
    "\n",
    "for pid in unique_pids:\n",
    "    hits.loc[hits.particle_id == pid, T_feature_names[:5]] = get_track_parameters(hits[hits.particle_id == pid]['x'].to_numpy(), hits[hits.particle_id == pid]['y'].to_numpy(), hits[hits.particle_id == pid]['z'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select certain eta region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_region = [-5,5]\n",
    "hits = hits[(hits.eta > eta_region[0]) & (hits.eta < eta_region[1])]\n",
    "# hits = hits.sort_values(by='eta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organise into adjacent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.arange(n_det_layers)\n",
    "layer_pairs = np.stack([l[:-1], l[1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['r', 'phi', 'z']\n",
    "feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, IDs = construct_graph(hits, layer_pairs=layer_pairs,\n",
    "                              phi_slope_max=phi_slope_max, z0_max=z0_max,\n",
    "                              feature_names=feature_names,\n",
    "                              feature_scale=feature_scale, T_feature_names = T_feature_names, T_feature_scale = T_feature_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v1.2.0-gpu [conda env:root] *",
   "language": "python",
   "name": "conda-root-pytorch-v1.2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
